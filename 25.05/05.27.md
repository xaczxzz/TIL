# TIL

### 개념 정리
- Moe, Model Merging 
- GPT3 in context Learning

### 3장 엘라스틱 모델 관리
- 엘라스틱 서치 클라우드 구현해서 실행

```python
from pathlib import Path
from eland.ml.pytorch import PyTorchModel
from huggingface_hub import cached_download
from eland.ml.pytorch.transformers import TransformerModel
from elasticsearch import Elasticsearch
from elasticsearch.client import MlClient
import getpass


es = Elasticsearch(
    "주소",
    api_key="api키"
)

hf_model_id='sentence-transformers/msmarco-MiniLM-L-12-v3'
tm = TransformerModel(model_id=hf_model_id, task_type="text_embedding") #torchType

es_model_id = tm.elasticsearch_model_id()
# es_model_id

tmp_path = "models"

Path(tmp_path).mkdir(parents=True, exist_ok=True)
model_path, config, vocab_path = tm.save(tmp_path)


ptm = PyTorchModel(es, es_model_id)
ptm.import_model(model_path=model_path, config_path=None, vocab_path=vocab_path, config=config)
# 일래스틱서치에 존재하는 모델 정보 조회
m = MlClient.get_trained_models(es, model_id=es_model_id)
m.body
s = MlClient.start_trained_model_deployment(es, model_id=es_model_id)
s.body
```
이런식으로 허깅페이스 모델을 연동해서 사용할 수 있다.