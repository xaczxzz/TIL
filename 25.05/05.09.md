# TIL

## 8장 효율적인 트랜스포머 구축

### 지식 정제 KD
> - 트랜스포머 모델을 구축할때 지연율, 모델의 크기, 정확도 등을 향상 시키기 위한 몇가지 방법에 대해 배운다.

- KD 지식 정제 방식 티처를 사용하여 스튜던트를 학습시키는 방법이다. 이떄 티처의 손실함수, 스튜던트의  손실함수를 이용하여 KLD을 통해 학습을 진행하게 된다.
```python
import torch.nn as nn
import torch.nn.functional as F
from transformers import Trainer

class DistillationTrainer(Trainer):
    def __init__(self, *args, teacher_model = None, **kwargs):
        super().__init__(*args,**kwargs)
        self.teacher_model = teacher_model
    def compute_loss(self,model, inputs, return_outputs=False,**kwargs):
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        inputs = inputs.to(device)
        outputs_stu = model(**inputs)
        # 스튜던트의 크로스 엔트로피 손실과 로짓을 추출
        loss_ce = outputs_stu.loss
        logits_stu = outputs_stu.logits
        # 티처의 로짓을 추출
        with torch.no_grad():
            outputs_tea = self.teacher_model(**inputs)
            logits_tea = outputs_tea.logits
        # 확률을 부드럽게하고 정제 손실을 계산 KLD
        loss_fct = nn.KLDivLoss(reduction="batchmean")
        loss_kd = self.args.temperature ** 2 * loss_fct(
            F.log_softmax(logits_stu / self.args.temperature, dim=-1),
            F.softmax(logits_tea/self.args.temperature, dim=-1)
        )
        loss = self.args.alpha * loss_ce + (1. -self.args.alpha) * loss_kd
        return (loss, outputs_stu) if return_outputs else loss


## 사용자 정의 트레이너를 만들었음.
## 티처와 동일한 스튜던트여야 학습이 잘된다.
```
이런식으로 Trainer 부분을 수정하여 작성 
네이버 플레이스 AI를 개발할 때도 KD 방식으로 sLLM 모델을 학습시켰다고 하였다.
### 양자화
> - 양자화는 여러번 사용을 해보았다. 이 단원에서 소개하는 양자화는 총 3가지의 방법이 존재한다.
> - 첫번째로 동적 양자화, 정적 양자화, 사전 훈련단계에서 양자화

- 동적 양자환는 추론 과정에서만 적용되는 단계이고, 즉석에서 양자화가 일어나기때문에 동적 양자화라고 불린다. 가장 단순하게 사용된다.
- 정적 양자화는 '동적 양자화'가 즉석에서 일어나 발생하는 여러 문제 , 지연 혹은 병목 현상을 막기 위해 사용된다. 

```python
## 양자화한 모델 불러오는 방법
model = AutoModelForSequenceClassification.from_pretrained(model)

model_quantized = quantize(model, ~~)

pipe = pipe("text-classification", model = model)

```
이 방법과 transformer
```python
# 이런식으로 트랜스포머에서 가져오는 양자화를 사용한다. Qlora등 학습 시킬때 잘 사용함
# 위에거는 동적인 양자화, 밑에는 추론 과정에서도 사용할 수 있다.
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)
```

### ONXX 프레임워크
> - 다음 방법은 ONXX 프레임워크를 이용한 가속화 방식이다. 
> - torch, tensorflow로 작성된 모델을 형식으로 ONXX 포멧으로 변경하여 최적화를 진행할 수 있다.

```python
from transformers.convert_graph_to_onnx import convert

model_ckpt = "transformersbook/distilbert-base-uncased-distilled-clinc"
onnx_model_path = Path("onnx/model.onnx")
convert(framework="pt", model=model_ckpt, tokenizer=tokenizer, 
        output=onnx_model_path, opset=12, pipeline_name="text-classification")


from onnxruntime import (GraphOptimizationLevel, InferenceSession, 
                         SessionOptions)

def create_model_for_provider(model_path, provider="CPUExecutionProvider"): 
    options = SessionOptions()
    options.intra_op_num_threads = 1
    options.graph_optimization_level = GraphOptimizationLevel.ORT_ENABLE_ALL
    session = InferenceSession(str(model_path), options, providers=[provider])
    session.disable_fallback()
    return session
```
이런식으로 기존의 모델형식으로 ONXX 형식으로 변경하여 최적화를 진행한다.

### 가중치 가지치기
> - 책에서 소개하는 방식은 2가지로서 절대값 가지치기, 이동 가지치기 방식을 설명한다.

> - 어떤 가중치를 가지치기하는 식으로 반복적으로 절댓값 가지치기를 적용하는 경우가 일반적이며, 그다음 원하는 희소성에 도달할 떄 까지 희소한 모델을 다시 훈련하는 방식으로 진행된다. 많은 계산량이 필요하다. 하지만 중요한 가중치 또한 같이 삭제되어 모델의 성능을 낮추는 방식이 될 수 있다.   

> - 이동 가지치기는 이러한 문제를 보완하고자 나온 방식이며, 원점으로부터 가장 크게 이동하는 가중치가 가장 중요하게 생각하며 학습을 진행한다. 또한 경사 하강법을 통해 학습하는 형식이다.
