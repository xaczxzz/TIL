# TIL

### 블로그 글 쓰기
> - BPE 관련 다른 모델은 어떻게 처리하는지 궁금하여 해당 내용들을 검색하여 블로그에 정리
***
### NER 한글 모델 구현하기
> - GPT가 책 기반으로 내준 문제 해결 
> - 한글로 NER 모델 파인튜닝하는 문제였다 
> - 제일 중요한 내용은 토큰화를 진행할때 ner_tags와 token을 매칭 시켜서 학습을 진행 해야 된다. 이때 token은 bpe에 적용되어 하나씩 분리되어 있는 상태

***
### 학습결과
> - 데이터안에 0(-100)으로 라벨링된 데이터가 너무 많이 있어 적은 랜덤으로 샘플링한 데이터셋으로 학습한 결과 성능이 좋지 않은 결과
> - 데이터셋의 양을 늘려서 학습을 진행하니, Loss도 잘 줄어들어 모델의 성능도 잘 보였다. 

**중요**
토큰화를 하는 과정에서 cls,eos 토큰을 넣고, 태깅 이외의 token에는 -100을 넣어 인식하지 못하게 데이터를 처리하는 과정이 이번 학습 과정의 핵심

