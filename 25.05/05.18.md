# TIL
### decoder gpt
> - gpt decoder 부분 코드로 구현하는 과정
> - openai/gpt2를 불러와서 한글데이터로 파인튜닝을 진행
```python
def generate_text(model, tokenizer, prompt, max_length=150, num_words=100):
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    outputs = model.generate(
        inputs["input_ids"],
        max_length=max_length,
        do_sample=True,
        top_k=50,
        top_p=0.9,
        num_return_sequences=1,
        pad_token_id=tokenizer.eos_token_id
    )
    text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    words = text.split()
    if len(words) > num_words:
        text = " ".join(words[:num_words])
    return text
```
Top_k, Top_p 적용
> - 실험 내용으로는 일반적으로 모델을 사용한거랑 한글 데이터를 학습시켜 생성의 차이가 있는지를 비교하였음.
> - 코드는 다 구현을 했는데 CUDA 메모리가 부족하여 학습을 진행하지 못함 -> 양자화를 거쳐 학습을 진행하였는데 openai/gpt2 from_pretrained에서는 적용되지 않는 문제가 발생
> - 아마도 model를 불러올떄 AutoModelForCausalLM이거와 bnbconfig가 호환이 되지 않아 발생한 에러로 예상이 된다.