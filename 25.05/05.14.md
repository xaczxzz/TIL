# TIL
## 10장 대규모 데이터 수집
> - 대규모 데이터를 이용하여 모델을 파인튜닝하는 과정을 진행하였음.    
파이썬 코드를 작성해주는 모델로 GPT 모델을 학습
> - 기본 데이터가 200GB, 압축시 50GB 너무 많은 데이터를 코드에서 불러오는 것도 시스템에 큰 영향을 미치게된다.
> - **스티리밍,매핑** 기술을 사용하여 동적으로 데이터를 할당하여 학습과정에 문제가 없게 하는 방법 load_dataset(data_name, streaming=True)
> - 또한 목적에 맞게 토크나이저를 다시 학습하여 사용

### 책을 기반으로 문제 해결하기 
개인적으로 다시 한번 책을 돌아보며 코드를 작성해보았다.
> - Distilbert를 이용하여 긍정,부정 파인튜닝하기
데이터는 빠른 결과를 확인하기 위해 500,500개를 사용하였고,
허깅페이스의 Trainer,IBMD 데이터등을 사용하여 코드 작성하였다.
```python
eval_results = trainer.evaluate()
accuracy = eval_results["eval_accuracy"] if "eval_accuracy" in eval_results else np.mean([p == l for p, l in zip(trainer.predict(test_tokenized_datasets).predictions.argmax(-1), test_tokenized_datasets["label"])])
print(f"정확도: {accuracy:.2%}")
# 81.6%의 정확도
```