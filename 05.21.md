# TIL

### torch 문법
```python
import torch
torch.no_grad() -> 역전파등 모델의 학습을 멈추는 상황 , requires_grad=True(역전파를 위한 설정) True 값을 False로 변경해줌
model.eval() -> 모델을 평가하기 위한 단계
```
> - topk 문법
```python
x = torch.rand(3, 4, 5)
y = torch.rand(3, 4, 5)

print(x)
print(y)
# x의 2번째 차원을 기준으로 가장 큰 값을 하나 뽑습니다.
values, indices = torch.topk(x, 1, dim=2)
# x에서 추출된 값과 똑같은 위치에서 y의 값을 하나 뽑습니다.
torch.gather(y, 2, indices)
```
> - expand,repect 문법
```python
# expand - 원본 tensor를 참조 (원본 바뀌면 애도 바뀜), repeat는 깊은 복사 tensor가 변경되더라도 값 변경이 발생 X

a = torch.rand(1,1,3)
print(a)
b = a.expand(4,-1,-1)
c = a.repeat(4,1,1)

print(b.shape)
print(c.shape)


a[0,0,0,] = 0
print(a)
print(b) # expand는 변경됨 0을 곱해서 
print(c) #repeat는 변경안됐음
```
> - dim에 따른 인덱싱 문법
```python
A = torch.arange(27).reshape(3,3,3) # 3,3 하면 에러 발생
# print(A)
indices = torch.tensor([
     [[0,0,0],
      [1,1,1],
      [2,2,2]],

    [  [1,1,1]
     ,[2,2,2,]
     ,[0,0,0]],

    [[0,1,2], # -> 대각선 3차원일떄 dim = 1은 대각선 , dim=2는 열 dim=0은 1차원 
     [0,1,2],
     [0,1,2]
     ]])

torch.gather(A,1,indices)
# A[i,indices[i,j,k],k] 이런식으로 인덱싱 들어감 (dim=1) , dim=2 aus A[i,j,indices[i,j,k]] 이런식으로 진행됨
```
> - Autograd 
자동 미분을 통해 변화량 파악 하는 torch 문법
```python
import torch
x = torch.ones(2, 2, requires_grad=True)
y1 = x + 2
print(y1)
# tensor([[3., 3.],
#         [3., 3.]], grad_fn=<AddBackward0>)

y2 = x - 2
print(y2)
# tensor([[-1., -1.],
#         [-1., -1.]], grad_fn=<SubBackward0>)

y3 = x * 2
print(y3)
# tensor([[2., 2.],
#         [2., 2.]], grad_fn=<MulBackward0>)

y4 = x / 2
print(y4)
# tensor([[0.5000, 0.5000],
#         [0.5000, 0.5000]], grad_fn=<DivBackward0>)
```
output의 값이 무조건 스칼라 값으로 나와야지 계산이 가능하다.
> - Optimizer loss,backward
2가지 방법이 존재
```python
# 데이터를 통해 학습을 진행할때
# 제일 많이 사용하는 형식으로 진행
# epoch 만큼 돌리고 , optimizer wight값 매번 초기화 해서 진행
# 가중치 매번 업데이트 하면서 진행
for i in range(5): #epoch 5번

    for num_train, (train_data,target_data) in enumerate(batches):
        optimizer.zero_grad()
        out = nn_model(train_data)
        loss = loss_function(out,target_data)
        loss.backward()
        loss_sum += loss.item()

        optimizer.step()
    loss_list.append(loss_sum/ (num_train+1))


# 다른 방법
# 모든 weight update를 한번에 진행함
# for epoch in range(5):
#     optimizer.zero_grad()
#     for ~~ 

```
> - dataloader 기법
```python 
# GPU를 이용하여 학습을 진행할떄 DataLoader를 사용할 경우 PageableMemory(CPU) -> Pinned Memory(GPU) -> DRAM(GPU) 이 과정에서 병목형상이 일어나는데,
# torch.util.data.DataLodaer(pin_memory=True)를 할 경우 Pin -> DRAM 으로 가서 병목현상 줄여줌

from transformers import TrainingArguments
training_args = TrainingArguments(
    dataloader_pin_memory=True # 이런식으로 사용됨
)

import torch.utils.data.dataloader

torch.utils.data.dataloader(pin_memory=True) # 이런식으로 

#non_blocking = Ture는 CPU -> GPU 복사과정을 비동기 전송으로 가능하게 한다. CPU 텐서가 pinned memory일 때만 작동
# pin_memory =True, non_blocking=False 동기 복사 (느리다)
# pin_memory =True, non_blocking=True 비동기 복사 (빠르다) 
# 이떄 .to('cuda') 에 경우 동기적 복사 GPU에 데이터 복사할때 까지 CPU가 기다린다. 
```
## 엘라스틱서치 내용
> - TF-IDF 
문서에 단어 빈도수를 기반으로 검색을 진행   
단어 빈도수를 검색하고 역 문서 빈도를 계산한다.

> - 문서 1,2,3 에 대해서 단어가 몇개 나왔는지 카운트

> - BM25