# TIL



## 1. 데이터 타입 옵션 (torch_dtype, load_in_4bit)
- **torch.float16 (FP16)**: 메모리 절약, 속도 빠름
- **torch.float32 (FP32)**: 정밀도 높음, 메모리 많이 사용
- **load_in_4bit**: 4비트 양자화, 극단적 메모리 절약 (bitsandbytes 필요)
- window일 경우 window 버젼에 맞게

**배운 점**
- 추론(Inference)에서는 **FP16**으로 충분함
- 학습(Training)에서는 **FP32/FP16 혼합**을 고려

## 2. LoRA 타겟 모듈 확인
### 방법
- `model.named_modules()`를 사용하여 **Polyglot-ko**의 모듈 이름 확인
- 예시: `query_key_value`, `dense_h_to_4h`

**배운 점**
- **LoRA 적용 시 모델 구조 이해가 필수**

## 3. DataCollatorForLanguageModeling
- 역할: 학습 데이터 배치 준비
- CLM용(`mlm=False`)으로 다음 토큰 예측 설정

**배운 점**
- **학습(Training)에만 필요**, 추론(Inference)에서는 사용하지 않음

## 4. 오늘의 핵심 인사이트
- **모델 학습 최적화**: LoRA와 FP16을 활용하여 효율적인 Fine-Tuning 가능
- **에러 디버깅**: 경로, 설정, 하드웨어 호환성 점검이 중요
- **추론 vs 학습**:
  - 추론(Inference): **FP16으로 간소화**
  - 학습(Training): **데이터 및 설정 관리 필수**

