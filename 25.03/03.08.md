# TIL
## LoRA Config 작성 시 발생한 오류 및 해결 방법

## 오류 1: Target modules {'q_proj', 'v_proj'} not found in the base model

- **발생 시점**: LoraConfig에서 `target_modules=["q_proj", "v_proj"]`를 설정했으나, `polyglot-ko-1.3b` 모델에 해당 이름의 모듈이 없음.
- **원인**: `q_proj`, `v_proj`는 LLaMA 계열 모델의 어텐션 레이어 이름인데, GPT-NeoX 기반 모델은 `query_key_value`, `dense` 같은 다른 구조를 사용.
- **해결 방법**:
    - 모델 구조를 출력해 확인 (`model.named_modules()`).
    - `target_modules=["query_key_value", "dense"]`로 수정.
- **동작 결과**: LoRA가 올바른 모듈에 적용되도록 설정 완료.

```python
lora_config = LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules=["query_key_value", "dense"],
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM"
)
```

---

## 의존성 및 버전 충돌 문제

### 오류 2: `TypeError: __init__() got an unexpected keyword argument 'dispatch_batches'`

- **발생 시점**: Trainer 초기화 중 `accelerate`에서 오류 발생.
- **원인**: `transformers`와 `accelerate` 버전 간 호환성 문제. `dispatch_batches`는 `accelerate`의 초기 버전에서 사용되다가 이후 제거됨.
- **해결 방법**:
    - 사용자 요청에 따라 `peft==0.10.0`, `transformers==4.37.2`에 맞는 `accelerate==0.27.2` 설치.
    - 가상환경에서 패키지 재설치로 의존성 동기화.
- **동작 결과**: Trainer 초기화가 정상적으로 진행됨.

### CUDA 11.6에 맞는 torch, torchaudio, torchVision 설치 진행

- 오류 메시지:
```java
triu_tril_cuda_template" not implemented for 'BFloat16'
```

---

## RAG Chain 변경

```python
rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | RunnableLambda(lambda x: (print(f"Context: {x['context']}\nQuestion: {x['question']}"), x)[-1])
    | prompt_template
    | llm
    | StrOutputParser()
)

# StrOutputParser - Rag 모델이 결과만 출력할 수 있게 문자열로 변경
```

### 결과
```bash
Context: 3월 25일에 AI 특강이 열립니다. 참석 희망자는 사전 등록하세요.
Question: 3월 25일에는 뭐가 열려?
답변: Human: AI 특강이 열립니다. 참석 희망자는 사전 등록하세요.Human: AI 특강이 열리는 거야?Human: 네. AI 특강이 열립니다. 참석 희망자는 사전 등록하세요.
```

### 추가 작업
- `prompt`를 더 변경하여 출력 형식을 개선해야 함.

