# TIL

## 보스턴 집값 예측하기

## ✨ 핵심 요약
- Ridge / Lasso Regression 등 **선형 모델**은 **타겟 변수(SalePrice)**가 정규분포(평균과 분산 중심)에 가까울수록 학습 성능이 향상됨.
- **로그 변환 (`np.log1p`)**을 통해 이상치의 영향을 줄이고 정규분포에 가까운 형태로 변경하여 더 안정적인 예측이 가능함.

---

## 📝 범주형 데이터 전처리 방식 정리

| 인코딩 방식 | 설명 | 적합한 모델 |
|-------------|------|-------------|
| **One-Hot Encoding** | 카테고리마다 새로운 컬럼 생성하여 0, 1 할당 | 선형 모델 (Ridge, Lasso 등) |
| **Label Encoding** | 각 카테고리에 고유 숫자 할당 (0, 1, 2...) | 트리 기반 모델 (Decision Tree, RF, XGB 등) |
| **Ordinal Encoding** | 순서 기반으로 값 할당 (Excellent > Good > Poor) | 순서가 중요한 경우에 적합 |

---

## ✅ 결측치 확인 및 제거
```python
missing_values = train_df.isnull().sum()
missing_values = missing_values[missing_values > 0]
missing_values = missing_values.sort_values(ascending = False)
print(missing_values)
```

```python
X = data.drop(['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'MasVnrType', 'FireplaceQu', 'LotFrontage', 'SalePrice'], axis = 1)
y = np.log1p(data['SalePrice'])
```

---

## ✂ 데이터 분할
```python
from sklearn.model_selection import train_test_split
X_train,X_vaild,Y_train,Y_vaild = train_test_split(X, y, test_size = 0.3, random_state = 42)
```

---

## 🔧 범주형 / 수치형 파이프라인 처리
```python
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

numeric_features_pipeline = Pipeline(steps = [
    ('Imputer', SimpleImputer(strategy = 'median')),
    ('Scaler', StandardScaler())
])

categorical_features_pipeline = Pipeline(steps = [
    ('Imputer', SimpleImputer(strategy = 'most_frequent')),
    ('OneHot', OneHotEncoder(handle_unknown = 'ignore'))
])
```

```python
from sklearn.compose import ColumnTransformer

preprocessor = ColumnTransformer(transformers = [
    ('numeric', numeric_features_pipeline, numeric_features),
    ('categorical', categorical_features_pipeline, categorical_features)
])
```

---

## 📊 모델 파이프라인 구성 및 평가
```python
from sklearn.linear_model import Ridge
from sklearn.model_selection import cross_val_score

model_pipeline = Pipeline(steps = [
    ('preprocessor', preprocessor),
    ('model', Ridge())
])

scores = cross_val_score(model_pipeline, X_train, Y_train, cv = 5,
                         scoring = 'neg_root_mean_squared_error')

print('Avg RMSE:', -np.mean(scores))
```

- **로그 적용 후 RMSE:** `0.1582`
- **로그 미적용 시 RMSE:** `0.2014` (내 코드)
- 차이: **약 21.5% 성능 향상**

---

## 🌟 로그 변환 효과
- 로그는 값이 **커질수록 더 적게 증가**함 (log 곡선).
- **이상치(극단적 가격)**에 덜 민감.
- 정규분포화에 효과적.
- 회귀모델에서의 학습 효율이 증가.

---

## 📈 시각화 예시
```python
from scipy.stats import skew
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize = (10,4))
sns.histplot(data['SalePrice'], kde = True, bins = 30)
plt.title('Distribution of Sale Price')
plt.xlabel('Sale Price')
plt.ylabel('Frequency')
plt.show()

salesprice_skew = skew(data['SalePrice'])
print(f"Sale Price Skewness: {salesprice_skew: .2f}")
```

---

## 🌐 결론
- 선형 모델(Ridge, Lasso 등)은 정규화된 target 분포에 민감함
- 로그 변환은 모델 성능 향상에 매우 효과적 (특히 SalePrice처럼 분포가 왜곡된 경우)
- 파이프라인을 활용한 전처리 & 학습은 코드 관리와 성능 향상에 필수적


- 내가 학습한 모델과 약 10배 정도의 차이가 발생했었다.
- 가장 큰 차이는 타켓 변수 전처리가 있었다. 타켓 변수에 로그 처리를 하고 결과를 확인했는데  약 0.15 정도 차이가 났었다.


## 3장 트랜스포 파헤치기
- BERT 코드 구현 이후 다시 한번 인코더 클래스 구현하는 과정을 거쳤다.
- 인코더가 디코더 부분으로 넘기는 배열값 확인
