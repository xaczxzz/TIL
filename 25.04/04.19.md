# TIL

### 4장 다중 언어 개체명 인식


**PAN-X** 데이터셋을 활용하였음 **WikiANN** 기반의 다국어 NER 데이터셋으로 구성되어 있다. **독일어(De), 프랑스어(Fr), 이탈리아어(It), 영어(En)** 를 대상으로 진행하였다. 

데이터셋은 HuggingFace `datasets` 라이브러리를 통해 불러왔으며, `Dataset` 객체로 관리하였다.

NER 태그의 분포를 시각화하기 위해 각 split(`train`, `validation`, `test`) 별로 등장한 개체명을 카운트하여 분포를 확인하였다.

```python
from collections import defaultdict, Counter

split2freqs = defaultdict(Counter)
for split, dataset in panx_ch.items():
    for row in dataset["ner_tags_str"]:
        for tag in row:
            if tag.startswith("B"):
                tag_type = tag.split("-")[1]
                split2freqs[split][tag_type] += 1
```
### 4.2 XLM-RoBERTa 모델 정의

XLM 모델은 RoBERTa 모델을 보고 학습시켜 구조는 동일하다.  
그래서 
```py
import torch.nn as nn
from transformers import XLMRobertaConfig
from transformers.modeling_outputs import TokenClassifierOutput
from transformers.models.roberta.modeling_roberta import RobertaModel
from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel
from transformers import RobertaConfig
class XLMRobertaForTokenClassification(RobertaPreTrainedModel):
    config_class = XLMRobertaConfig
    def __init__(self, config, *inputs, **kwargs):
        super().__init__(config, *inputs, **kwargs)
        self.num_labels = config.num_labels
        self.roberta = RobertaModel(config,add_pooling_layer=False)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)
        self.classifier = nn.Linear(config.hidden_size,config.num_labels)
        ## 가중치 초기화
        self.init_weights()

    def forward(self,input_ids=None, attention_mask=None, token_type_ids=None,labels=None,**kwargs):
        outputs = self.roberta(input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids,**kwargs)

        sequence_output = self.dropout(outputs[0])
        logits = self.classifier(sequence_output)
        loss = None
        if labels is not None:
            loss_fct = nn.CrossEntropyLoss()
            loss = loss_fct(logits.view(-1,self.num_labels), labels.view(-1))

        return TokenClassifierOutput(loss=loss,logits=logits, hidden_states=outputs.hidden_states,attentions=outputs.attentions)
        '''
        TokenClassifierOutput
        loss: Optional[torch.FloatTensor] = None
        logits: torch.FloatTensor = None
        hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None
        attentions: Optional[Tuple[torch.FloatTensor, ...]] = None

        '''

```
이런식으로 모델을 구현해서 config에 담아 모델 구조를 선언할 수 있다.

```py
import torch
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
xlmr_model = (XLMRobertaForTokenClassification
              .from_pretrained(xlmr_model_name,config=xlmr_config).to(device)
              )
```

### 미세튜닝 과정
다음 과정에는 학습 과정이다. Trainer클래스를 불러와서 클래스와, 나머지 config을 작성해준다.
```py
from transformers import Trainer

trainer = Trainer(model_init=model_init,args=training_args, data_collator=data_collator,compute_metrics=compute_metrics,
                  train_dataset=panx_de_encoded["train"],
                  eval_dataset=panx_de_encoded["validation"],
                  tokenizer=xlmr_tokenizer)
```

## 한국어 NER 모델 학습하기

>  나는 여기서 더 나아가 한국어 모델을 NER 태스크를할 수 있게 튜닝을 진행해보았다.
기존 다국어 XLM 모델에서 한국 RoBERTa 모델을 불러와 학습을 진행했다.

### 한국어 데이터 가져오기

```py
from datasets import load_dataset

ds = load_dataset("kimmeoungjun/korean_ner")

```

```py
text = "오늘은 농구 이겨야하는데"
print(tokenizer_klue(text))

#{'input_ids': [0, 3822, 2073, 7124, 25281, 2205, 13964, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}
```
토큰화 하는 과정을 확인할 수 있음.


### 데이터 NER 토큰 확인하기
```PY
from collections import Counter
import pandas as pd
nested_tags = ds['train']['ner_tags']

flat_tags = [tag for sublist in nested_tags for tag in sublist]


tag_counts = Counter(flat_tags)

df = pd.DataFrame(tag_counts.items(), columns=["Tag", "Count"]).sort_values(by="Count", ascending=False)
print(df)

      Tag   Count
2       O  729659
1  B-MISC  185995
0   B-PER   43034
3   B-ORG   40860
5  I-MISC   33098
4   B-LOC   20881
6   I-PER    5165
7   I-ORG    4668
8   I-LOC     211

```

다음으로 모델 구조는 동일하니 해당 구조를 그대로 사용, 대신 
**config_class = RobertaConfig** 로 변경

### 모델 훈련 및 성능 확인

```PY
from torch.nn.functional import cross_entropy
from tqdm import tqdm

def forward_pass_with_label(batch):
    features = [dict(zip(batch, t)) for t in zip(*batch.values())]
    batch = data_collator_klue(features=features)
    input_ids = batch["input_ids"].to(device)
    attention_mask = batch["attention_mask"].to(device)
    labels = batch["labels"].to(device)

    with torch.no_grad():
        output = trainer.model(input_ids, attention_mask)
        predicted_label = torch.argmax(output.logits, axis=-1).cpu().numpy()

    loss = cross_entropy(output.logits.view(-1, 9),
                        labels.view(-1), reduction="none")
    loss = loss.view(len(input_ids), -1).cpu().numpy()

    return {"loss": loss, "predicted_label": predicted_label}
    
    
    from tqdm import tqdm
klue_encoded_valid = klue_encoded_valid.map(forward_pass_with_label,
                                            batched=True,
                                            batch_size=32,)

df = klue_encoded_valid.to_pandas()

```

> 제일 성능이 좋지 않았던것은 LOC에 대한 위치 태그 데이터 성능이 좋지 않았음.
