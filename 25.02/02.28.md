# 📌 TIL

## 📝 개요
LangChain 프레임워크를 사용해 **RAG(Retrieval-Augmented Generation)** 기술을 구현하고, 한국어 데이터를 처리하기 위한 실험을 진행할 예정이다. 한글 임베딩과 LLM을 활용해 벡터 DB(FAISS) 기반의 검색 및 답변 생성 기능을 테스트해본다.

---

## 🛠️ 기술 스택
### 1. LangChain
- **설명**: LLM과 외부 데이터를 연결하는 프레임워크.
- **역할**: 벡터 DB에서 검색된 데이터를 LLM에 전달해 문맥 기반 답변 생성.


### 2. 한글 임베딩
- **모델**: `nlpai-lab/KoE5`
- **특징**: 한국어 텍스트를 고품질 벡터로 변환. KoBERT나 KoELECTRA 대비 최신 성능 우수.
- **사용 이유**: 한국어 문서의 의미를 정확히 벡터화해 검색 품질 향상.

### 3. LLM 모델
- **모델**: `EleutherAI/polyglot-ko-1.3b`
- **특징**: 1.3B 파라미터의 한국어 특화 오픈소스 LLM. 경량화된 다국어 모델.
- **사용 이유**: 한국어 생성 능력 우수, 로컬 환경 테스트 가능.

### 4. 벡터 DB
- **구현**: `FAISS` (Facebook AI Similarity Search)
- **특징**: 고속 유사도 검색을 지원하는 오픈소스 벡터 DB.
- **목적**: 문서를 벡터로 저장하고, 질의 기반 검색으로 관련 정보 추출.

---

## 🚀 실험 계획
### 1. 데이터 준비
- 한국어 문서 수집 (게시판 더미 데이터).
- 문서를 청크 단위로 분할.

### 2. 임베딩 생성
- `nlpai-lab/KoE5`로 문서 청크를 벡터화.
- LangChain의 `HuggingFaceEmbeddings` 모듈 활용.

### 3. 벡터 DB 구축
- FAISS에 생성된 벡터 데이터 저장.
- 샘플 질의로 검색 기능 테스트.

### 4. LLM 연동
- `EleutherAI/polyglot-ko-1.3b`를 LangChain의 `RetrievalQA`로 연결.
- FAISS에서 검색된 문맥을 입력으로 사용해 답변 생성.

### 5. 결과 평가
- 검색된 문서와 생성된 답변의 정확도 점검.
- 한국어 처리 성능 및 응답 속도 확인.

---

## 🌟 기대 효과
- 한국어 문서 기반의 정확한 검색 및 답변 생성 가능성 확인.
- RAG를 통해 LLM의 최신성 부족 문제 보완.
- FAISS의 고속 검색으로 실시간 응답 구현 가능.

---

## 🔧 향후 과제
- 더 큰 한국어 데이터셋으로 테스트 확장.
- 다른 한글 임베딩 모델(예: KoELECTRA) 또는 LLM(예: KoGPT)과 성능 비교.
- FAISS의 검색 속도 및 스케일링 최적화.

---

## 💡 느낀 점
LangChain과 FAISS를 활용한 RAG 구현은 한국어 데이터 처리에서 실용적인 접근이 될 가능성을 보여준다. `KoE5`와 `polyglot-ko-1.3b` 조합이 기대 되어 결과가 좋다면 GPU 로 돌려볼 예정이다.
